<!DOCTYPE html>
<html>
  <head>
    <!-- Niklas Buschmann 2015 MIT <http://github.com/niklasbuschmann> -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    
    <meta name="description" content="ah!">
    <title>Blog: Machine Learning Equations by Saurabh Verma</title>
    <link rel="canonical" href="http://www-users.cs.umn.edu/~verma//">
    <link href="/main.css" rel="stylesheet" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,200italic,300italic,400italic,600italic,700italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Gentium+Basic:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css' rel='stylesheet' type='text/css'>
    <link type="application/atom+xml" rel="alternate" href="http://www-users.cs.umn.edu/~verma//feed.xml" title="Blog: Machine Learning Equations by Saurabh Verma" />
    
    
	
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-78615814-1', 'auto');
  ga('send', 'pageview');

</script>

  </head>
  <body>
    <header>
      <h1><a href="/">Blog: Machine Learning Equations by Saurabh Verma</a></h1>
      <nav>
        <ul>
          <li><a href="/">Home</a></li><li><a href="/about.html">About</a></li><li><a href="/archive.html">Archive</a></li>
        </ul>
      </nav>
    </header>
    
    
    
    <article>
      <header>
        <h2><a href="/paperlist/2016/05/31/why-autoencoder-sparse.html"> Blog: Why Regularized Auto-Encoders learn Sparse Representation?</a></h2>
        <p><time datetime="2016-05-31T17:37:00-05:00">May 31, 2016</time> • PaperList</p>
      </header>
      <div>
<p>This paper has some great insights to offer in design of autoencoders. As title suggests, it addresses “whether regularized helps in learning sparse representation of the data theoretically?”</p>

<p><strong>Here is the setup:</strong> we have an input <script type="math/tex">\mathbf{x} \in \mathbf{R}^{d}</script> which is mapped to latent space <script type="math/tex">\mathbf{h=s(Wx+b)}</script> via autoencoder where <script type="math/tex">\mathbf{s}</script> is encoder activation function, <script type="math/tex">\mathbf{W} \in \mathbf{R}^{m\times n}</script> is the weight matrix, and <script type="math/tex">\mathbf{b} \in \mathbf{R^m}</script> is the encoder bias and <script type="math/tex">\mathbf{h} \in \mathbf{R^m}</script> is hidden representation or outputs of hidden units.</p>

<p>For analysis, paper assumes that decoder is linear i.e. <script type="math/tex">\mathbf{W^{T}h}</script> decodes back the encoded hidden representation and loss is squared loss function. Therefore, for learning <script type="math/tex">\mathbf{W,b}</script> parameters of autoencoder; objective function is:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{split}
J_{AE} &=\mathbb{E_x}[(\mathbf{x-W^Th})^2] \\
&= \mathbb{E_x}[(\mathbf{x-W^Ts(Wx+b)})^2]\\
\end{split}
\end{equation} %]]></script>

<blockquote>
  <p>Now we are interested in sparsity of <script type="math/tex">\mathbf{h}</script>, hidden representation.</p>
</blockquote>

<p>Besides above, paper makes two more assumptions.</p>

<blockquote>
  <p><strong>Assumption 1</strong>: Data is drawn from a distribution <script type="math/tex">\mathbf{x} \sim  X</script> for which <script type="math/tex">\mathbb{E_x}[\mathbf{x}]=0</script> and <script type="math/tex">\mathbb{E_x}[\mathbf{xx^T]=I}</script>, where <script type="math/tex">\mathbf{I}</script> is identity matrix.</p>
</blockquote>

<p>Basically, it assumes that data is whitened which is reasonable for some cases. There is one more assumption from analysis point of view which is needed for the derivation of theorems, we will see that later.</p>

<p>Finally, for a give data sample <script type="math/tex">\mathbf{x}</script>, each hidden unit <script type="math/tex">h_j</script> gets activated if pre-activation unit <script type="math/tex">a_j</script> is greater than <script type="math/tex">\delta</script> threshold:</p>

<script type="math/tex; mode=display">a_j=W_j\mathbf{x}+b_j</script>

<script type="math/tex; mode=display">h_j=s(a_j)</script>


      </div>
      <footer>
        <p><a href="/paperlist/2016/05/31/why-autoencoder-sparse.html">read more &raquo;</a></p>
      </footer>
    </article>
    
    <article>
      <header>
        <h2><a href="/paperlist/2016/05/29/convgraph.html">Blog: Learning Convolutional Networks for Graphs</a></h2>
        <p><time datetime="2016-05-29T17:37:00-05:00">May 29, 2016</time> • PaperList</p>
      </header>
      <div>
<p>For past couple of months, I have been wondering about how convolutional networks can be applied to  graphs. As we know, convolutional networks have became the state of the art in image classification and also in natural language processing. It is easy to see how convolutional networks  exploits the locality and translation invariance properties in an image. People have now also realized on how to exploit those properties in NLP using convolutional networks efficiently. It is time to look at other, more general, domains such as  <em>graphs</em> where notion of locality or receptive field needs to be defined. At this point, we can start thinking about graph neighborhood concepts as it is going to play a major role in connecting with receptive fields of convolutional networks.</p>


      </div>
      <footer>
        <p><a href="/paperlist/2016/05/29/convgraph.html">read more &raquo;</a></p>
      </footer>
    </article>
    
    <article>
      <header>
        <h2><a href="/paperlist/2016/05/28/ReadingListICML16.html">Reading List: ICML Papers 2016 </a></h2>
        <p><time datetime="2016-05-28T17:37:00-05:00">May 28, 2016</time> • PaperList</p>
      </header>
      <div>
<p>To make a habit of reading more papers, I have decided to write comments, may be some quick or sometime detail, on  the papers which I find interesting and related to my research area. Hopefully, this will come handy in solving my own research problems.</p>

<p><strong>Starting with ICML 2016 Papers:</strong></p>

<ul>
  <li>Revisiting Semi-Supervised Learning with Graph Embeddings ICML 2016</li>
  <li>Why Regularized Auto-Encoders learn  Sparse Representation? ICML 2016</li>
  <li>On the Consistency of Feature Selection With Lasso for Non-linear Targets. ICML 2016</li>
  <li>Additive Approximations in High Dimensional Nonparametric Regression via the SALSA. ICML 2016</li>
  <li>The Variational Nystrom method for large-scale spectral problems. ICML 2016</li>
  <li>A Kernelized Stein Discrepancy for Goodness-of-fit Tests and Model Evaluation. ICML 2016 (Possibly a new way to measure difference in probability distribution other than most common – KL divergence )</li>
  <li>Low-Rank Matrix Approximation with Stability. ICML 2016</li>
  <li>Unsupervised Deep Embedding for Clustering Analysis. ICML 2016</li>
  <li>Online Low-Rank Subspace Clustering by Explicit Basis Modeling. ICML 2016</li>
  <li>Community Recovery in Graphs with Locality. ICML 2016</li>
  <li>Analysis of Deep Neural Networks with Extended Data Jacobian Matrix. ICML 2016</li>
  <li>Stochastic Variance Reduced Optimization for Nonconvex Sparse Learning. ICML 2016</li>
  <li>Compressive Spectral Clustering ICML. 2016</li>
  <li>Variance-Reduced and Projection-Free Stochastic Optimization. ICML 2016</li>
  <li>Learning Convolutional Neural Networks for Graphs. ICML 2016</li>
  <li>Discrete Deep Feature Extraction: A Theory and New Architectures.  ICML 2016</li>
</ul>

      </div>
      <footer>
        <p><a href="/paperlist/2016/05/28/ReadingListICML16.html">read more &raquo;</a></p>
      </footer>
    </article>
    
    
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <footer>
      <span><a href="http://www-users.cs.umn.edu/~verma/">Saurabh Verma</a></span>
      <span><a href="https://github.com/me/"><i class="fa fa-github-square"></i></a><a href="https://twitter.com/me/"><i class="fa fa-twitter-square"></i></a><a href="https://plus.google.com/+me"><i class="fa fa-google-plus-square"></i></a><a href="https://www.reddit.com/user/me/"><i class="fa fa-reddit-square"></i></a><a href="https://www.youtube.com/user/me"><i class="fa fa-youtube-square"></i></a><a href="http://steamcommunity.com/id/me/"><i class="fa fa-steam-square"></i></a></span>
      <span>&copy; 2016</span>
    </footer>
  </body>
</html>
