<!DOCTYPE html>
<html>
  <head>
    <!-- Niklas Buschmann 2015 MIT <http://github.com/niklasbuschmann> -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    
    <meta name="description" content="ah!">
    <title> Why Regularized Auto-Encoders learn Sparse Representation? › Machine Learning Equations</title>
    <link rel="canonical" href="http://www-users.cs.umn.edu/~verma//paperlist/2016/05/29/why-autoencoder-sparse.html">
    <link href="/main.css" rel="stylesheet" type="text/css">
    <link href='//fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,200italic,300italic,400italic,600italic,700italic' rel='stylesheet' type='text/css'>
    <link href='//fonts.googleapis.com/css?family=Gentium+Basic:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css' rel='stylesheet' type='text/css'>
    <link type="application/atom+xml" rel="alternate" href="http://www-users.cs.umn.edu/~verma//feed.xml" title="Machine Learning Equations" />
    <script src="//saurabhML.disqus.com/embed.js" async></script>
    
	
    <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-78615814-1', 'auto');
  ga('send', 'pageview');

</script>

  </head>
  <body>
    <header>
      <h1><a href="/">Machine Learning Equations</a></h1>
      <nav>
        <ul>
          <li><a href="/">Home</a></li><li><a href="/about.html">About</a></li><li><a href="/archive.html">Archive</a></li>
        </ul>
      </nav>
    </header>
    
    
    <article>
      <header>
        <h2><a href="/paperlist/2016/05/29/why-autoencoder-sparse.html">Why Regularized Auto-Encoders learn Sparse Representation?</a></h2>
        <p><time datetime="2016-05-29T17:37:00-05:00">May 29, 2016</time> • PaperList</p>
      </header>
      <div>
<p>This paper has some great insights to offer in design of autoencoders specially “does regularized helps in learning sparse representation of the data?”</p>

<p>Here is the setup: we have an input <script type="math/tex">\mathbf{x} \in \mathbf{R}^{d}</script> which is mapped to latent space <script type="math/tex">\mathbf{h=s(Wx+b)}</script> via autoencoder where <script type="math/tex">\mathbf{s}</script> is encoder activation function, <script type="math/tex">\mathbf{W} \in \mathbf{R}^{m\times n}</script> is the weight matrix, and <script type="math/tex">\mathbf{b} \in \mathbf{R^m}</script> is the encoder bias and <script type="math/tex">\mathbf{h} \in \mathbf{R^m}</script> is hidden representation or outputs of hidden units.</p>

<p>For analysis, paper assumes that decoder is linear i.e. <script type="math/tex">\mathbf{W^{T}h}</script> decodes back the encoded hidden representation and loss is squared loss function. Therefore, for learning <script type="math/tex">\mathbf{W,b}</script> parameters of autoencoder objective function is:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\begin{split}
J_{AE} &=\mathbb{E_x}[(\mathbf{x-W^Th})^2] \\
&= \mathbb{E_x}[(\mathbf{x-W^Ts(Wx+b)})^2]\\
\end{split}
\end{equation} %]]></script>

<blockquote>
  <p>We are interested in sparsity of <script type="math/tex">\mathbf{h}</script>, hidden representation.</p>
</blockquote>

<p>Besides above, paper makes two more assumptions.</p>

<blockquote>
  <p><strong>Assumption 1</strong>: Data is drawn from a distribution <script type="math/tex">\mathbf{x} \sim  X</script> for which <script type="math/tex">\mathbb{E_x}[\mathbf{x}]=0</script> and <script type="math/tex">\mathbb{E_x}[\mathbf{xx^T]=I}</script>, where <script type="math/tex">\mathbf{I}</script> is identity matrix.</p>
</blockquote>

<p>Basically, it assumes that data is whitened which is reasonable for some cases. There is one more assumption from analysis point of view which is needed for derivation of theorems, we will see that later.</p>

<p>Finally, for a give data sample <script type="math/tex">\mathbf{x}</script>, each hidden unit <script type="math/tex">h_j</script> gets activated if pre-activation unit <script type="math/tex">a_j</script> is greater than <script type="math/tex">\delta</script> threshold:</p>

<script type="math/tex; mode=display">a_j=W_j\mathbf{x}+b_j</script>

<script type="math/tex; mode=display">h_j=s(a_j)</script>

<p>Let’s further</p>

<p>The idea is to somehow show that pre-activation units <script type="math/tex">a_j, \forall j</script> for majority of samples is less than <script type="math/tex">\delta</script> threshold. Why we want that? It would mean that</p>

      </div>
      
      
      <div id="disqus_thread"></div>
      <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      
    </article>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <footer>
      <span><a href="http://www-users.cs.umn.edu/~verma/">Saurabh Verma</a></span>
      <span><a href="https://github.com/me/"><i class="fa fa-github-square"></i></a><a href="https://twitter.com/me/"><i class="fa fa-twitter-square"></i></a><a href="https://plus.google.com/+me"><i class="fa fa-google-plus-square"></i></a><a href="https://www.reddit.com/user/me/"><i class="fa fa-reddit-square"></i></a><a href="https://www.youtube.com/user/me"><i class="fa fa-youtube-square"></i></a><a href="http://steamcommunity.com/id/me/"><i class="fa fa-steam-square"></i></a></span>
      <span>&copy; 2016</span>
    </footer>
  </body>
</html>
